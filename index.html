<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search.">
  <meta name="keywords" content=" Vision-Language-Action Model, Monte Carlo Tree Search, Test-time RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLA-Reasoner: Empowering Vision-Language-Action Models with
Reasoning via Online Monte Carlo Tree Search</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./multimedia/figures/robot.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/KevinGuo07">
      <span class="icon">
          <i class="fab fa-github"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/KevinGuo07">Wenkai Guo</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://guanxinglu.github.io/">Guanxing Lu</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="https://denghaoyuan123.github.io/">Haoyuan Deng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://gary3410.github.io/">Zhenyu Wu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://andytang15.github.io/">Yansong Tang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>1†</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup>†</sup>Corresponding author</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanyang Technological University</span>
            <span class="author-block"><sup>2</sup>Tsinghua Shenzhen International Graduate School</span>
            <span class="author-block"><sup>3</sup>Beijing University of Posts and Telecommunications</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2509.22643"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.22643"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/KevinGuo07/VLA-Reasoner"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./multimedia/figures/teaser_01.png" alt="VLA-Reasoner Teaser" style="width: 100%;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">VLA-Reasoner</span> empowers vision-language-action models with reasoning capabilities through online Monte Carlo Tree Search.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named <span class="dnerf">VLA-Reasoner</span> that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling.
          </p>
          <p>
            Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. 
          </p>
          <p>
            We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Method Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content">
          <img src="./multimedia/figures/pipeline_01.png" alt="VLA-Reasoner Pipeline" style="width: 100%;">
          <p class="has-text-justified">
            The overall pipeline of VLA-Reasoner. At test time, a lightweight and modified MCTS searches for the optimal action conditioned on the VLA prediction. The search is steered by expert-like sampling and dense reward shaping, which guide expansion and backup throughout the tree. The method is plug-and-play, and it can be attached to any VLA-based manipulation policy and consistently improves performance across tasks, environments, and robot embodiments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method Overview -->

    <!-- Experimental Results -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experimental Results</h2>
        
        <!-- Simulation Results -->
        <div class="content" style="margin-bottom: 3rem;">
          <h3 class="title is-4">Results in Simulations</h3>
          <img src="./multimedia/figures/quanlitative_results.png" alt="Simulation Results" style="width: 100%;">
          <p class="has-text-justified" style="margin-top: 1rem;">
            Average success rates across 500 episodes for LIBERO and 100 episodes for SimplerEnv. Our method outperforms OpenVLA-SFT on all 4 direction tasks and Octo-Small/SpatialVLA on 4 tasks. Bold entries mark the highest success rates, underlined for second-best. Asterisked results are chosen baselines and locally evaluated for fairness.
          </p>
        </div>

        <!-- Real World Results -->
        <div class="content">
          <h3 class="title is-4">Results in Real World</h3>
          <img src="./multimedia/figures/realworld_results.png" alt="Real World Results" style="width: 45%; margin: 0 auto; display: block;">
          <p class="has-text-justified" style="margin-top: 1rem;">
            Average success rates of 5 tasks in different scenarios. Each task is evaluated 20 times. Our method apparently improves OpenVLA and π0-FAST in all tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Experimental Results -->
  </div>
</section>


<!-- Video Demonstrations -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Performance Comparison -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Performance Visualization</h2>
        <div class="content has-text-justified">
          <p>
            Comparison between VLA-Reasoner and baseline methods on stack cube task. Our method demonstrates more robust and successful manipulation.
          </p>
        </div>

        <div class="columns is-centered">
          <div class="column">
            <div class="content">
              <h3 class="title is-5">VLA-Reasoner (Ours)</h3>
              <video id="stack-cube-reasoner" autoplay controls muted loop playsinline width="100%">
                <source src="./multimedia/videos/stack_cube_reasoner.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="column">
            <div class="content">
              <h3 class="title is-5">π0-FAST (Baseline)</h3>
              <video id="stack-cube-pifast" autoplay controls muted loop playsinline width="100%">
                <source src="./multimedia/videos/stack_cube_pifast_fail.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Performance Comparison -->

    <hr style="margin: 3rem 0;">

    <!-- Long Horizon Manipulation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Long Horizon Manipulation</h2>
        <div class="content has-text-justified">
          <p>
            VLA-Reasoner successfully handles complex multi-step manipulation tasks that require careful planning and sequential reasoning.
          </p>
        </div>

        <div class="columns is-centered">
          <div class="column is-three-fifths">
            <video id="stack-2cups" autoplay controls muted loop playsinline width="100%">
              <source src="./multimedia/videos/stack_2cups_reasoner.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Long Horizon Manipulation -->

    <hr style="margin: 3rem 0;">

    <!-- Spatial Generalization -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Spatial Generalization</h2>
        <div class="content has-text-justified">
          <p>
            VLA-Reasoner demonstrates strong spatial generalization capabilities, adapting to varied object positions.
          </p>
        </div>

        <div class="columns is-centered">
          <div class="column is-three-fifths">
            <video id="pick-grape" autoplay controls muted loop playsinline width="100%">
              <source src="./multimedia/videos/pick_grape_reasoner.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Spatial Generalization -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{guo2025vla,
  title={Vla-reasoner: Empowering vision-language-action models with reasoning via online monte carlo tree search},
  author={Guo, Wenkai and Lu, Guanxing and Deng, Haoyuan and Wu, Zhenyu and Tang, Yansong and Wang, Ziwei},
  journal={ICRA},
  year={2026}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/KevinGuo07/VLA-Reasoner" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
